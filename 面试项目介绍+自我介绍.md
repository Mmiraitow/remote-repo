### 项目介绍

#### 播放器

我基于FFmpeg实现了一款音视频播放器，采用播放器核心和UI分离的方式设计播放器，并采用多线程编程，将各个模块解耦，通过ffmpeg实现对媒体文件的解封装和解码，实现了倍速播放、播放位置跳转和播放暂停控制等功能。

#### 流媒体推流

实现从本地推流到流媒体服务器，包括屏幕和麦克风录制、然后对采集到的数据的编码封装与推流，同时还实现了超时处理和消息反馈机制来实时监测推流情况

#### 项目开发中遇到的问题

在考虑怎么跟UI解耦，各功能之间怎么解耦

播放器  参考开源方案ijkmediaplayer完善



推流中就是出现问题短时间内不知道哪里出了问题，阻塞也不知道阻塞到哪里，延迟也不知道具体哪里

后续考虑更高程度的跨平台、采集音频用alsa、

去跟RTMP推流

尝试自己实现一个RTSP服务器



### 自我介绍

面试官您好，我叫徐朝阳，目前就读于华中科技大学硕士二年级，很荣幸能够参加本次面试，我的目标求职方向是C++和音视频开发，在项目方面：我完成了音视频播放器和流媒体推流项目的开发，希望能够有机会加入贵公司继续学习成长，以上是我的自我介绍，谢谢！

### 具体细节

只记录之前没记录的

#### 准备工作

​	stream_open：

​	初始化SDL、初始化包、帧队列、初始化时钟、初始化音量、创建解复用线程和视频刷新线程

#### 退出工作

​	stream_close:

​	解复用线程退出：join（等待线程退出）

​	终止解码器线程

​	音频还要关闭SDL输出、释放重采样器

​	销毁解码器（释放解码器上下文）

​	释放包、帧队列，以及释放其他申请的内存空间

#### UI与播放器核心分离

​	通过一个消息循环线程完成UI和播放器核心之间的交互，然后去举例子，比如seek和截图，connect函数绑定信号和事件，事件发送消息到队列，循环线程中不断从队列中读取消息，然后判断消息体的类型并执行该事件

#### 多线程编程

​	实现各个模块之间的解耦，分为解复用线程（数据读取线程）、视频解码线程、音频解码线程、视频刷新线程、音频回调输出

##### 解复用（数据读取）线程

​	将不同的数据流分离出来

​	识别、流提取和解封装

​	创建解复用上下文avformat_alloc_context

​	打开文件（探测协议类型，如果是网络文件则创建网络连接）

​	探测媒体类型（封装格式、编码信息） avformat_find_stream_info

​	av_find_best_stream选择流

​	 stream_component_open：打开对应解码器并创建相应解码线程

​	 av_read_frame(ic, pkt)从流中读取packet包，并送到相应包队列

##### 视频刷新线程

​	用了QT的组件，同步到音频去播放

##### 解码线程

​	

##### 音频输出

​	跟其他的不一样，没有new一个新线程

```
在stream中SDL_Init
在audio_open中定义音频参数（数据格式S16）并指定回调函数
回调函数填充数据到缓冲区
stream_component_open->audio_open->SDL_OpenAudio
stream_component_open->SDL_PauseAudio(0)
```

#### 音量大小调节

SDL库函数SDL_MixAudio()，混合音频数据和静音缓冲区数据，**在音频回调函数中进行**

#### 包队列缓存阈值设置

帧数过多会占用太多内存

解复用线程中，读取媒体数据之前进行一个判断，音视频包大小总和大于阈值或各自有足够的数据后休眠10ms

根据队列首尾的时间戳，相减得到buffer时长/帧数和帧间隔相乘计算，用后者校验前者

根据buffer的水位调整播放速率？？

#### 注意网络流的读取

1、url打不开，怎样反馈

**打印日志分析**

2、只收到音频数据没有视频怎么办：打印收到第一帧视频的时间和收到第一帧音频的时间，相差太大说明推流端出问题（或者是服务端出问题）

3、分析推流端是否正常推数据：分别记录第一次采集到数据、第一次送到编码器、第一次获取到视频编码、第一次推流的时间，得到每个环节产生的延迟，可能是编码延迟





#### 为什么选择以音频为基准同步

从同步的复杂性来说，视频可通过丢帧或重复来同步视频；而音频的调整更复杂，简单的丢帧或重复会影响听觉，需要考虑**变速**的方式处理样本数量

音频包通常比视频包要小，因此音频可以更快的加载并播放，视频需要更多的时间缓冲，网络不好时至少可以保证听到音频



#### 对解码线程的控制

考虑到B帧，receive_frame不能马上读P帧出来，要继续sendB帧，且先返回B，再返回P，这样才符合顺序



#### Sonic变速不变调的原理

改变采样率可以实现变速但是也会变调

声波是有可以看作是有无数个不同频谱、振幅和相位的正弦波组成，音调的大小主要取决于声波基频的高低

Sonic变调不变速的主要原理通过使用**波形相似叠加算法**并采用**AMDF（平均幅度差函数法）**方法来寻找最相似的第三帧来实现变速不变调

加速：按照一定比例合并 保证平滑

减速：插入数据0



#### 创新？

后续可以引入WebRTC，基于ijkplayer的快直播传输层SDK



#### 播放器怎样播放不同采样率的音频帧

核心就是重采样，重采样成SDL2支持的格式



#### seek的精度问题

ffmpeg中的API avformat_seek_file  精度只能做到GOP间隔，即两个I帧的间隔，**因为seek后需要找到相应的I帧位置才能开始解码**

例如从前往后seek到3100ms，而I帧在2000ms，则实际seek的地方是2000ms的位置

**如何更精确的seek**：在调用avformat_seek_file后继续处理，即在[2000, 3000)之间解码出来的图像帧不显示，解码后丢掉，直到检测到时间戳接近3100ms的帧再开始显示（**注意是接近，没法一定精确到3100，因为不一定有时间戳为3100的帧**）



#### OpenGL渲染图片

使用CPU处理与渲染图像数据会占据CPU大量的资源,相对于CPU,GPU更适合处理简单重复并发数多的任务。

OpenGL是用GPU渲染图像的一种方案

渲染管线：固定渲染管线->可编程管线

**后面再了解，现在有更需要了解的**



#### 硬件解码

用专门的硬件设备解码音视频，而不是依赖软件在CPU上执行解码任务，

设备包括：GPU、专用解码器、CPU也有硬件解码功能

专门的硬件加速器和算法

**在处理高分辨率和高码率的视频流时，硬件解码的优势更明显**



**硬和软的区别：**

硬：利用硬件，可提高解码速度，但可能会牺牲画质，设备限制可能无法支持所有的解码格式

软：通过软件模拟解码过程，更灵活（支持更多格式），画质可能会好点、但会造成CPU的负担，导致卡顿



#### YUV图片大小

1920*1080 的YUV420P，YUV420P默认是8位位深，所以一个Y、U、V占一个字节，但四个Y共用一个UV分量，所以平均每个像素（4+1+1）/4 = 1.5字节



#### YUV相比RGB的优势

亮度和色度分离，可以对亮度和色度单独调整

色度信息包含较少的细节，可以有更高的压缩率，显著减小数据量





#### 封装

打包到一个容器文件，为了组织和同步音视频流



#### MP4不知道moov位置如何快速起播

需要优化文件结构，moov中存储了音视频轨道信息和解码信息（**通常位于文件尾部？**）

解决办法：

1、moov box前置，可以使用**ffmpeg中的命令-movflags faststart重新封装MP4文件**，将moov box移到前面

​	为了实现在线播放（边下载边播放），也需要把moov信息提前

2、对长视频使用f（Fragmented片段）MP4格式，基于MP4拓展的，为更方便网络传输，

​	fMP4格式将媒体文件拆成多个独立的分片，每个分片都有自己的元数据（moof）和媒体数据（mdat）



#### 解复用中做的工作

1、容器识别，以flv，mp4为例，有一个封装格式的识别函数，发现前三个字符是FLV就认为是flv文件

​	MP4 通过box的识别，比如 m o o v

2、流提取

​	flv根据Tag类型（在tag header中）去区分音视频

3、解封装

​	将媒体流解封装为大量的packet

4、读取媒体信息 find_stream_info  (mp4可以不调用，mp4有box存储编码信息)

5、find_best_stream查找指定类型的流

6、av_read_frame读取数据包（packet）



#### H264 SPS PPS在mp4文件中的位置

moov->视频轨道中的媒体信息部分（mdia）->minf->stbl->**avcC**

**H265**：hvcC 或hev1



### 没搞懂的点

seek和时间戳/时间基设置



